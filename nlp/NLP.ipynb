{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "This notebook show simple Fasttext model trained on E. A. Poe's, Mary Shelley's, and HP Lovecraft's books.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import string\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import en_core_web_sm\n",
    "import fasttext\n",
    "import gensim.models.fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns;\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Corpus creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my corpus I decided to use [this](https://www.kaggle.com/c/spooky-author-identification) dataset from Kaggle as I won't have to bother too much with cleaning and splitting the sentences. It contains texts by Edgar Allan Poe, Mary Shelley, and HP Lovecraft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('corpus/corpus_raw.csv')\n",
    "dataset = dataset.drop('Unnamed: 0', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def cleanup_text(docs, logging=False):\n",
    "    _stopwords = set(stopwords.words('english'))\n",
    "    texts = []\n",
    "    for doc in tqdm(docs):\n",
    "        doc = nlp(doc, disable=['parser', 'ner', 'tagger', 'textcat'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in _stopwords and tok not in string.punctuation]\n",
    "        tokens = ' '.join(tokens)\n",
    "        if len(tokens) == 0:\n",
    "            tokens = np.nan\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)\n",
    "\n",
    "dataset['cleaned_text'] = cleanup_text(dataset['text'])\n",
    "dataset = dataset.dropna(axis='rows').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(path, serie):\n",
    "    with Path(path).open(mode='w') as fw:\n",
    "        fw.write('\\n'.join(serie))\n",
    "\n",
    "save_corpus('corpus/corpus_clean.txt', dataset['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to go with [fastText](https://fasttext.cc) as it is able to work better with unseen words and mainly becuase I have already used it in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised('corpus/corpus_clean.txt', model='skipgram', wordNgrams=3, epoch=25, dim=100)\n",
    "model.save_model('model.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = gensim.models.fasttext.load_facebook_model('model.mod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter((' '.join(dataset['text'])).split())\n",
    "word, count = zip(*word_counter.most_common(10))\n",
    "data = {'word': word, 'count': count}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='word', y='count', data=df).set_title('10 most common words in raw dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter((' '.join(dataset['cleaned_text'])).split())\n",
    "word, count = zip(*word_counter.most_common(10))\n",
    "data = {'word': word, 'count': count}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='word', y='count', data=df).set_title('10 most common words in cleaned dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two previous plots show the importance of cleaning the data (mostly removing the stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, 100), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=15).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following plot shows most similar and most opposite to word `raven` (from EA Poe's work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(vecs, 'raven', [i[0] for i in vecs.wv.most_similar(negative='raven', topn=15)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following plot shows most similar and most opposite to word `frankenstein` (from Mary Shelleys's work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(vecs, 'frankenstein', [i[0] for i in vecs.wv.most_similar(negative='frankenstein', topn=15)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following plot shows most similar and most opposite to word `cthulhu` (from HP Lovecraft's work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(vecs, 'cthulhu', [i[0] for i in vecs.wv.most_similar(negative='cthulhu', topn=15)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing downloaded story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm gonna download sample from Stephen King's Hearts In Atlantis and replace some words with it most similar and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://authorpages.hoddersystems.com/StephenKing/sample1.asp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "if r.status_code != 200:\n",
    "    print('Unable to download the sample!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('\\s+')\n",
    "texts = list(map(lambda t: t.font.text, bs.find_all('p', attrs={'align': 'left'})))\n",
    "text = ' '.join(texts)\n",
    "text = regex.sub(' ', text)\n",
    "print('Original text')\n",
    "print('=' * 10)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = {'father': vecs.wv.most_similar('father', topn=1)[0][0],\n",
    "               'mother': vecs.wv.most_similar('mother', topn=1)[0][0],\n",
    "              'gravestone': vecs.wv.most_similar('gravestone', topn=1)[0][0],\n",
    "              'bike': vecs.wv.most_similar('bike', topn=1)[0][0],\n",
    "              'phone': vecs.wv.most_similar('phone', topn=1)[0][0]}\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(_text, translation):\n",
    "    text = str(_text)\n",
    "    for k, v in translation.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "print('Modified text')\n",
    "print('=' * 10)\n",
    "print(translate(text, translation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to get the most similar words to selected 5 words: `father, mother, gravestone, bike, phone` and replace them in the original text. The resulting text definitely lost its original meaning by replacing `father` with `playmate`, `mother` with `child` and `bike` with `crackle`. On the other hand, `gravestone` and `phone` didn't really lost their meaning by replacing with the most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = {'father': vecs.wv.most_similar('father', topn=2)[1][0],\n",
    "               'mother': vecs.wv.most_similar('mother', topn=2)[1][0],\n",
    "              'gravestone': vecs.wv.most_similar('gravestone', topn=2)[1][0],\n",
    "              'bike': vecs.wv.most_similar('bike', topn=2)[1][0],\n",
    "              'phone': vecs.wv.most_similar('phone', topn=2)[1][0]}\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = {'father': vecs.wv.most_similar('father', topn=3)[2][0],\n",
    "               'mother': vecs.wv.most_similar('mother', topn=3)[2][0],\n",
    "              'gravestone': vecs.wv.most_similar('gravestone', topn=3)[2][0],\n",
    "              'bike': vecs.wv.most_similar('bike', topn=3)[2][0],\n",
    "              'phone': vecs.wv.most_similar('phone', topn=3)[2][0]}\n",
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not even going to try to replace words with ther 2nd and 3rd most similar words, it can be seen from the translation dictionaries that the resulting text wont make any sense at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But I am going to try to replace each word with its most similar (except for stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = text.lower().translate({ord(\"'\"): \"\", ord(\",\"): \"\", ord(\".\"): \"\", ord(\"-\"): \" \", ord(\"(\"): \"\", ord(\")\"): \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = {word: vecs.wv.most_similar(word, topn=1)[0][0] for word in preprocessed_text.split() if word not in set(stopwords.words('english'))}\n",
    "\n",
    "translate(preprocessed_text, translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, it can be seen that this doesn't even make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm model.mod\n",
    "!rm corpus/corpus_clean.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
